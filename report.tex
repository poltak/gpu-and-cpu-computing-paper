\documentclass[a4paper,11pt]{article}

\usepackage[hidelinks]{hyperref}
\usepackage[font={small}]{caption}
\usepackage{graphicx}
\usepackage[lined,boxed]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{poltakmacros}           % Personal macros included in file 'poltakmacros.sty'.
\usepackage{geometry}

\geometry{margin=2cm}


\author{Jonathan Poltak Samosir}
\title{FIT3143 Assignment 2}


\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
\end{abstract}
\smallskip
\noindent \textbf{Keywords.} blah, blah, blah
\newpage
\pagenumbering{roman}
\tableofcontents
\newpage

\pagenumbering{arabic}

% TODO: Split sections into different files
\section{Introduction} % (fold)
\label{sec:introduction}

% section introduction (end)


\section{Introduction to the multi-core CPU} % (fold)
\label{sec:introduction_to_the_multi_core_cpu}

\subsection{The central processing unit} % (fold)
\label{sub:the_central_processing_unit}
The central processing unit --- hereby referred to as the CPU --- is a fundamental piece of hardware within modern computers that handles
the fetching, decoding and execution of each instruction of a computer program~\cite{web:CPUWiki}. It handles the processing
of these instructions through the use of two key components: the arithmetic logic unit --- hereby referred to as the ALU
--- for basic arithmetical and logical operations, and the control unit --- hereby referred to as the CU --- for the input/output
operations between the CPU and memory, along with the overall control of the fetch, decode and execute cycle. Given the
physical hardware constraints, only a single instruction may be processed at any given time by a CPU.
% subsection the_central_processing_unit (end)

\subsection{The multi-core CPU} % (fold)
\label{sub:the_multi_core_cpu}
To define the multi-core CPU, it is perhaps important to first define the term ``core''. Like many terms in the area of computing,
the term ``core'' is often misused or used with many different meanings attached to it. For the purposes of this paper, the
term ``core'' simply refers to actual CPUs as defined in~\sectref{sub:the_central_processing_unit}.

Given this definition of a core, the multi-core CPU can simply be defined as a piece of computing hardware that contains
two or more cores. As the multi-core CPU contains more than one core, rather than being constrained to processing a single
instruction at any given time, in theory it is possible to be processing $n$ instructions on an $n$-core CPU at any given time.

Given what multi-core CPU hardware makes possible, since their introduction to consumers in the early 2000s, the multi-core
CPU has changed the way programmers design their programs to take advantage of hardware. Programs that are designed with
code concurrency in-mind now have advantages that were only ever available previously on systems that have multiple physical
CPU chips.
% subsection the_multi_core_cpu (end)

\subsection{Multi-core CPU architecture} % (fold)
\label{sub:multi_core_cpu_architecture}
The newest evolutions in multi-core CPU architecture families in modern consumer computing are arguably Intel's
``Haswell'' and AMD's ``Steamroller'' lines of processors.

Intel's Haswell is the fourth generation of their ``Core'' line of multi-processors, that places a larger focus on reducing power-consumption and improving performance of their on-chip graphics processing unit, or GPU~\cite{web:ForbesHaswell}. This point, specifically, will be elaborated on in further detail in later sections.

AMD's Steamroller architecture is the third evolution of AMD's ``Bulldozer''
architecture, a direct competitor to Intel's Haswell architecture implemented
in their ``Core'' line of multi-core processors. Steamroller placed a larger focus on improving parallelism between the available cores~\cite{web:AMDCCCSlides}, while also attempting to address competitor Intel's area of dominance--- power
consumption.
% subsection multi_core_cpu_architecture (end)

\subsection{Intel Haswell Core i7-4770K architecture} % (fold)
\label{sub:intel_haswell_architecture}
As an example of a Haswell CPU to specifically look at, we will take Intel's
current flagship enthusiast model Haswell quad-core processor, the Core i7-4770K. While having four cores all on the same chip, this Core i7 also
contains its own GPU on the chip. The Haswell i7 is a multi-level cache CPU, with each core having three levels of cache memory
available to it; from fastest to slowest: L1, L2 and L3. Each core has its own
dedicated 64 KB of L1 cache (split into 32 KB halves for separate data and instruction caches), 256 KB of L2 cache and all the cores share a much slower 8 MB of L3 cache~\cite{web:TomsHWCorei7}. The overall layout of the chip, as explained here, can be seen in~\figref{fig:haswell-layout}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{img/haswell-layout}
  \caption{The Intel Core i7-4770K quad-core CPU die layout.~\cite{web:TomHWCorei7DiePic}}
\label{fig:haswell-layout}
\end{figure}

Something of note is how the i7-4770K enables pipelining of operations with its hardware. Before proceeding into pipeling, it is important to touch on the differences between instructions and operations: Instructions are decoded by the CPU into atomic parts which are known as micro-operations which use and require specific resources. These micro-operations are then queued into the ``Unified Reservation Station'' (URS) where they wait until their resources are available. The micro-operations are then dispatched from the URS to the appropriate execution port. This is all done ``out of order'' where operations are just dispatched whenever possible, rather than in a pre-determined order when there is no resource dependencies between operations~\cite{book:Corei7PerfAnalysis}. Hence, this is the specific implementation of CPU pipelining that is enabled on the Haswell architecture. As can be seen from~\figref{fig:haswell-new-ports}, Haswell has eight distinct execution ports, each supporting different operations. This allows a maximum of eight independent micro-operations to be executed each clock cycle~\cite{web:TomsHWCorei7}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{img/new-ports}
  \caption{Shows how operation out of order execution pipelining works in the Intel Core i7-4770K~\cite{web:TomsHWCorei7}. The Unified Reservation Station can be seen along with the i7's eight execution ports. New hardware additions to the Haswell architecture highlighted.}
\label{fig:haswell-new-ports}
\end{figure}
% subsection intel_haswell_architecture (end)

\subsection{AMD Steamroller/Kaveri architecture} % (fold)
\label{sub:amd_steamroller_architecture}
As an example of a Steamroller-based CPU, we will look at AMD's current flagship A10-7850K processor, which is based on the third evolution of the Steamroller architecture, codenamed ``Kaveri''.

Something of note when looking at modern AMD processors is to note AMD's use of the marketing term ``APU'' --- short for Accelerated Processing Unit. This is essentially AMD's term for a GPU and CPU dual-offering on the same die~\cite{web:AMDAPU}, similar to what is provided by Intel's Haswell line of processors (as elaborated on in~\sectref{sub:intel_haswell_architecture}).

Like the Haswell i7, the Kaveri processors have four cores. Unlike the Haswell, the cores are split into two distinct ``dual-core'' units (each, of course, housing two separate cores). As the Kaveri line are also multi-level cache processors, each dual-core unit has its own L1 and L2 cache. The L1 cache is split into a 96 KB instruction cache for each dual-core unit and four separate 16 KB of data cache reserved for each of the cores~\cite{web:Guru3dKaveri}. The L2 cache is 2 MB large for each dual-core unit. Unlike the Haswell processors, there is no L3 cache present.

Like the cache architecture, the die layout for the Kaveri processors, seen in~\figref{fig:kaveri-layout}, is quite different to what we have previously seen with Intel's Haswell processors. The most obvious difference is that the majority of the die is taken up by the GPU. This shows AMD's larger emphasis on competitive graphics with their ``APU'' direction~\cite{web:WikiAPU}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{img/kaveri_die}
  \caption{The AMD Kaveri quad-core CPU die layout.~\cite{web:Guru3dKaveri} Note, the DDR3 PHY unit refers to the main memory controller.}
\label{fig:kaveri-layout}
\end{figure}
% subsection amd_steamroller_architecture (end)

% section introduction_to_the_multi_core_cpu (end)



\section{Programming the multi-core CPU} % (fold)
\label{sec:programming_the_multi_core_cpu}

\subsection{Thinking parallel} % (fold)
\label{sub:thinking_parallel}
Having multiple cores available on a single chip greatly expands what is possible with the way programs are implemented and run on computers. As explained earlier in~\sectref{sub:the_multi_core_cpu}, on an $n$=core CPU, it is possible to be processing $n$ instructions concurrently at any given time.

This concurrent execution of instructions that is possible with multi-core CPUs brings the possible benefits of parallel computing, and in-turn programming, to a far greater number of computers, and thus programmers.
Before multi-core CPUs were available, parallel computing was reserved to more special-purpose computers with multiple physical CPUs (on different physical chips as opposed to a single die).

While having the hardware there to support parallel computing on the one machine, the hardware cannot magically take the all the instructions from a program and spread them out perfectly across all the cores in a system to have a fully parallelised program. This is for a number of reasons, but a lot of the time it comes back to a primitive problem being because the underlying algorithm used in a program was not designed with parallelism in mind; everything is very sequential and imperative, often because that's the way the person the algorithm was designed by learned to think about these problems.
Programming for multi-core CPUs not only requires a different style of programming, it requires the programmer to shift their mindset to think in a parallel fashion.
% subsection thinking_parallel (end)

\subsection{Example of a \vs{sequential}{parallel} algorithm} % (fold)
\label{sub:example_of_a_vs}
Take the task of doing a simple matrix multiplication on the following matrices, $matrix_A$ and $matrix_B$:

\begin{displaymath}
matrix_A =
  \begin{pmatrix}
    a_1 & b_1 & c_1 \\
    a_2 & b_2 & c_2
  \end{pmatrix}
\end{displaymath}

\begin{displaymath}
matrix_B =
  \begin{pmatrix}
    a_1 & b_1 \\
    a_2 & b_2 \\
    a_3 & b_3
  \end{pmatrix}
\end{displaymath}

If you were to do this on pen and paper, most people would probably do it following a method similar to the algorithm
shown in~\algoref{algo:matrix_mult_seq}. This method works fine for how we would naturally do it, therefore a lot of
programmers who do not know any better would perhaps write code with a similar underlying algorithm to this. That is fine,
and the computer would get the correct answer eventually after computing all $T$ the dot products, placing them where they
are meant to go in $matrix_C$, so let's say in $T$-time. Tough thinking about the problem some more, one can see that each dot product calculation
is completely independent of any other dot product calculation, only needing to share read access of values from the input
matrices. As the outcome of these dot product calculations are independent of one-another, if there were two different people,
or computers, working on the problem with $T$ dot products at the same time, we could save roughly half the calculation time by splitting the
number of dot products that need to be calculated between the two of them, letting us do the work in $T/2$-time. Now take the same problem but this time with
$n$ available computers to compute dot products. If we could split the total number of dot products, $T$, evenly between
the $n$ computers, it would be possible to get a $T/n$ speed-up in compute time. This is what is meant by computing in
parallel and what is very much possible with multi-core CPUs. A parallel version of~\algoref{algo:matrix_mult_seq} is
shown in~\algoref{algo:matrix_mult_para}.

\begin{algorithm}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

  \Input{Two matrices, $matrix_A$ and $matrix_B$}
  \Output{A single matrix, $matrix_C$}
  \BlankLine
  \ForEach{column $C$ in $matrix_B$}{
    \ForEach{row $R$ in $matrix_A$}{
      get dot product of $C$ and $R$
    }
  }
\caption{A sequential high-level algorithm for multiplying matrices.}
\label{algo:matrix_mult_seq}
\end{algorithm}

\begin{algorithm}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwProg{Fn}{Function}{is}{end}
\SetKwFunction{FnWorker}{calculateDotProducts}

  \Input{Two matrices, $matrix_A$ and $matrix_B$, pool of workers, $workerPool$}
  \Output{A single matrix, $matrix_C$}
  \BlankLine
  $numWorkers = getNumAvailableWorkers()$

  $numDotProducts = getNumDotProducts()$

  \BlankLine
  \ForEach{worker $W$ in $workerPool$}{
    $dotProducts[] = getNewDotProducts(numDotProducts/numWorkers)$

    call calculateDotProducts(dotProducts[]) on $W$
  }

  \BlankLine
  \BlankLine
  \Fn{\FnWorker{dotProductCalculations[]}}{
    \ForEach{dot product calculation $DP$ in $dotProductCalculations$}{
      perform $DP$
    }
  }
\caption{A parallel high-level algorithm for multiplying matrices.}
\label{algo:matrix_mult_para}
\end{algorithm}

Something of note is that there are limits to the benefits the parallel algorithm has over the sequential algorithm.
If the number of workers is greater than the number of dot product calculations that need to be performed, there
are going to be some workers that have nothing to do. This means that any increase in the number of workers after this point
is meaningless in-terms of making computations faster.

Furthermore, if there is only a single worker, then the computations of dot products are essentially all going to be performed
sequentially, just as in the sequential algorithm.
% subsection example_of_a_vs (end)

\subsection{Models for enabling of parallel and concurrent programming} % (fold)
\label{sub:enablers_for_programming_in_parallel}
The idea and theory behind the benefits of programming in parallel is all well and good, although there need to be some
underlying programming models that to enable these parallel algorithms to be implemented.

\subsubsection{Multiple threads of execution} % (fold)
\label{ssub:threads_execution}
One notable, and very widely
used, method to allow concurrent and, in-turn, parallel programming is through the use of threads. Threads are now a core
underlying part of all widely-used modern operating systems. Threads allow the programmer to specify multiple threads
of execution to run concurrent to one another to perform some particular logic. At a lower-level of the operating system,
beyond the area of the application programmer's concern, the kernel handles the scheduling of all these different concurrent threads
to be run on different cores if possible. Running the threads of different cores means that they can be run truly in parallel
rather than concurrently sharing processor time on a single core. There are numerous APIs available to applications programmers
in different programming languages
allowing them to implement threads in their applications allowing the possibility of parallel programs on multi-core systems.
Some popular APIs include:
\begin{itemize}
  \item POSIX Threads (commonly referred to as ``Pthreads'')
  \item Java's ``java.util.concurrent'' package
  \item .NET's Thread Pool API
\end{itemize}
% subsubsection subsubsection_name (end)

\subsubsection{Message passing between processes} % (fold)
\label{ssub:message_passing_between_processes}
The concept of message passing is simply that; the passing of messages between processes. Message passing as a concept is
not exactly a model to allow parallel programming, although used indirectly, it makes concurrent and parallel programming
possible. If there are multiple processes running on a multi-core machine, perhaps some processes on different cores, by
operating system design, they are generally completely isolated from one another in terms of instructions and data. Message
passing lets these independent processes communicate and thus similar concurrency that we've seen in threads can be achieved.
Another important topic related to parallel computing in general, although not directly to multi-core CPUs, is that a
properly configured message passing system can allow parallel distributed computing, for example over networks, and abstract
it in such a way that the programmer need not be concerned about the host computer for particular processes. One notable
message passing system that does exactly this is Message Passing Interface (MPI), which has become somewhat of an unofficial
standard for message passing.
% subsubsection message_passing_between_processes (end)

% subsection enablers_for_programming_in_parallel (end)

% section programming_the_multi_core_cpu (end)



\section{Introduction to the GPU} % (fold)
\label{sec:introduction_to_the_gpu}

% section introduction_to_the_gpu (end)


\section{Differences in programming the GPU} % (fold)
\label{sec:differences_in_programming_the_gpu}

% section differences_in_programming_the_gpu (end)


\section{Parallelism in the multi-core CPU and GPU} % (fold)
\label{sec:parallelism_in_the_multi_core_cpu_and_gpu}

% section parallelism_in_the_multi_core_cpu_and_gpu (end)


\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)


\bibliographystyle{acm}
\bibliography{report}

\end{document}
